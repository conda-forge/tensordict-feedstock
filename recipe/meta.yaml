{% set name = "tensordict" %}
{% set version = "0.10.0" %}

package:
  name: {{ name|lower }}
  version: {{ version }}

source:
  # NOTE:
  # The PyPI address does not have any package-source available yet
  # Once the source is added there, change the recipe to build with
  # the source from PyPI.
  url: https://github.com/pytorch-labs/tensordict/archive/v{{ version }}.tar.gz
  sha256: e11984847f755ba97f29c318a232286defb5a4678f439151ff3a1baf1c221d3e

build:
  number: 2
  script: sed -i -e 's/dynamic = \["version"\]/version={{ version }}/g'  pyproject.toml; {{ PYTHON }} -m pip install . -vv --no-deps --no-build-isolation
  skip: true  # [win]

requirements:
  build:
    - {{ compiler('cxx') }}
    - {{ compiler('c') }}
    - {{ stdlib("c") }}
    - cmake
    - make
    - ninja
  host:
    - python
    - setuptools
    - wheel
    - pytorch
    - pip
  run:
    - python
    - pytorch
    - numpy
    - cloudpickle
    - packaging
    - orjson
    - importlib-metadata
    - pyvers

test:
  imports:
    - tensordict
  commands:
    - pip check
  requires:
    - pip

about:
  home: https://github.com/pytorch-labs/tensordict
  summary: TensorDict is a pytorch dedicated tensor container.
  license: MIT
  license_file: LICENSE
  description: |
    `TensorDict` is a dictionary-like class that inherits properties from tensors,
    such as indexing, shape operations, casting to device or point-to-point communication
    in distributed settings.

    The main purpose of TensorDict is to make code-bases more _readable_ and _modular_ by
    abstracting away tailored operations:

    ```python
    for i, tensordict in enumerate(dataset):
        # the model reads and writes tensordicts
        tensordict = model(tensordict)
        loss = loss_module(tensordict)
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()
    ```

    With this level of abstraction, one can recycle a training loop for highly heterogeneous task.
    Each individual step of the training loop (data collection and transform, model prediction,
    loss computation etc.) can be tailored to the use case at hand without impacting the others.
    For instance, the above example can be easily used across classification and segmentation tasks,
    among many others.

    PyPI: [https://pypi.org/project/tensordict/](https://pypi.org/project/tensordict/)

    ---

    ### **Conda-Forge Package Made with Conda-Forger App**: :zap:

    :fire: The conda-forge recipe was generated with [Conda-Forger App](https://sugatoray-conda-forger.streamlit.app/).

    :point_right: [![Streamlit App](https://static.streamlit.io/badges/streamlit_badge_black_white.svg)](https://sugatoray-conda-forger.streamlit.app/)

    [_streamlit-conda-forger-app]: https://sugatoray-conda-forger.streamlit.app/

  doc_url: https://pytorch-labs.github.io/tensordict/
  dev_url: https://github.com/pytorch-labs/tensordict

extra:
  recipe-maintainers:
    - sugatoray
    - jan-janssen
